{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from architecture import *\n",
    "import torch\n",
    "from data import get_dataloader\n",
    "from utils import create_sampler\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from functions import *\n",
    "from PT_plus_MAP import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset.\n",
    "# Execute the notebooks in dataset to download the data and prepare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Architecture ###\n",
    "# model = MLP(num_layers=2, num_feat=512, num_classes=61)\n",
    "# model = Conv1d(num_layers=2, num_feat=512, num_classes=61)\n",
    "# model = GNN(num_classes=61)\n",
    "\n",
    "# Only the needed variables will be used to create the model.\n",
    "model_name = 'GNN'  #'MLP', 'GNN' or 'Conv1d'\n",
    "num_classes = 64  # Number of classes in the base dataset. If you reproduce the results of the article, use 64 for split1 and 65 for split2.\n",
    "num_layers = 1  # Number of hidden layers.\n",
    "num_feat = 128  # For 'MLP'/'GNN', number of features per hidden layer. For 'Conv1d', number of feature maps per hidden layer.\n",
    "n_step = 1  # Only for 'GNN'. Number of times the input signal is diffused. See the 'GNN' architecture for more details.\n",
    "\n",
    "### Optimization ###\n",
    "lr = 0.1\n",
    "weight_decay = 1e-4\n",
    "lr_gamma = 0.1\n",
    "n_epoch = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\n",
    "model_parameters = get_model_parameters(model_name, num_layers, num_feat, num_classes, n_step)\n",
    "model = get_model(model_name, model_parameters)\n",
    "\n",
    "# Data parameters\n",
    "### TO UPDATE\n",
    "IBC_path = '/bigdisk2/nilearn_data/neurovault/collection_6618/'\n",
    "split_dir = '../dataset/split/'\n",
    "parcel, batch_size = get_variables(model.name)\n",
    "\n",
    "# Episodes parameters\n",
    "n_episode = 500\n",
    "n_way = 5\n",
    "n_shot = 5\n",
    "n_query = 15\n",
    "\n",
    "# Save path\n",
    "save_path = f'./results/{model.name}/'\n",
    "\n",
    "# Sampler\n",
    "train_sampler = create_sampler(IBC_path, split_dir, 'train')\n",
    "val_sampler_infos = [n_episode, n_way, n_shot, n_query]\n",
    "\n",
    "# Loader\n",
    "train_loader = get_dataloader('train', IBC_path, parcel, split_dir, meta=False, batch_size=batch_size, sampler=train_sampler)\n",
    "val_loader = get_dataloader('val', IBC_path, parcel, split_dir, meta=True, sampler_infos=val_sampler_infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer, scheduler = get_optimizer(model, lr, n_epoch)\n",
    "\n",
    "# Move on CUDA.\n",
    "use_cuda = True\n",
    "model = model.cuda()\n",
    "    \n",
    "# Number of epochs between each evaluation on the validation dataset.\n",
    "iter_val_acc = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model and evaluate it on the validation set at each epoch.\n",
    "epochs_acc = []\n",
    "epochs_loss = []\n",
    "epochs_train_acc = []\n",
    "\n",
    "best_acc = -1\n",
    "for epoch in range(n_epoch+1):\n",
    "    # Train for one epoch.\n",
    "    epoch_loss, epoch_train_acc = train(model, criterion, optimizer, train_loader, use_cuda)\n",
    "    print(\"\\rLoss at epoch {}: {:.2f}.\".format(epoch+1, epoch_loss), end='')\n",
    "    print(\"(Acc \\t: {:.2f}).\".format(epoch_train_acc*100),end='')\n",
    "    # Evaluate on the few-shot tasks from the validation set.\n",
    "    if epoch % iter_val_acc == 0:\n",
    "        optimizer.zero_grad()\n",
    "        epoch_acc = episodic_evaluation(model, val_loader, val_sampler_infos, use_cuda)\n",
    "        print(\"Acc on tasks at epoch {}: {:.2f}.\".format(epoch+1, epoch_acc*100))\n",
    "    scheduler.step()\n",
    "    # Check if it is the best epoch_acc so far.\n",
    "    is_best = epoch_acc > best_acc\n",
    "    best_acc = max(epoch_acc, best_acc)\n",
    "    \n",
    "    # Save the model\n",
    "    epochs_loss.append(epoch_loss)\n",
    "    epochs_train_acc.append(epoch_train_acc*100)\n",
    "    epochs_acc.append(epoch_acc*100)\n",
    "    save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': \"MLP - num_layers {} - num_feat {} - num_classes {}\".format(num_layers, num_feat, num_classes),\n",
    "                'state_dict': model.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'epochs_acc': epochs_acc,\n",
    "                'epochs_train_acc': epochs_train_acc,\n",
    "                'epochs_loss': epochs_loss \n",
    "            }, is_best, folder=save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the training has been correct.\n",
    "def load_statistics(save_path, type='last'):\n",
    "    if type == 'best':\n",
    "        checkpoint = torch.load('{}/model_best.pth.tar'.format(save_path))\n",
    "    elif type == 'last':\n",
    "        checkpoint = torch.load('{}/checkpoint.pth.tar'.format(save_path))\n",
    "    else:\n",
    "        assert False, 'type should be in [best, or last], but got {}'.format(type)\n",
    "    epochs_acc = checkpoint['epochs_acc']\n",
    "    epochs_train_acc = checkpoint['epochs_train_acc']\n",
    "    epochs_loss = checkpoint['epochs_loss']\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    return epochs_acc, epochs_loss, epochs_train_acc, best_acc\n",
    "\n",
    "epochs_acc, epochs_loss, epochs_train_acc, best_acc = load_statistics(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(epochs_train_acc, 'g', label='train')\n",
    "plt.plot(epochs_acc, 'r', label='val')\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()\n",
    "plt.plot(epochs_loss, 'g')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n",
    "\n",
    "print(\"The best accuracy on tasks on the validation set is: {:.2f}.\".format(best_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loader\n",
    "train_loader = get_dataloader('train', IBC_path, parcel, split_dir, meta=False, batch_size=1, sampler=None)\n",
    "test_loader = get_dataloader('test', IBC_path, parcel, split_dir, meta=False, batch_size=1, sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "acc_1_shot,conf1, acc_5_shot, conf5 = do_extract_and_evaluate_simplified(model, train_loader, test_loader, save_path)\n",
    "acc_1_shot, conf1, acc_5_shot, conf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_loader = get_dataloader('test', IBC_path, parcel, split_dir, meta=False, batch_size=1, sampler=None)\n",
    "acc_1_shot, conf1, acc_5_shot, conf5 = do_extract_and_evaluate_simplified_PT_plus_MAP(model, test_loader, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_1_shot, conf1, acc_5_shot, conf5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myriam",
   "language": "python",
   "name": "myriam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
